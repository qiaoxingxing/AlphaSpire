import json
import re
import pandas as pd
import yaml
from pathlib import Path

from utils.config_loader import ConfigLoader
from utils.text_dealer import truncate_text

# --- è·¯å¾„ ---
BASE_DIR = Path(__file__).resolve().parents[1]
PROMPT_FILE = BASE_DIR / "prompts" / "template_generating.yaml"
FIELDS_DIR = BASE_DIR / "data" / "wq_fields"
TEMPLATE_FIELDS_FILE = BASE_DIR / "data" / "wq_template_fields" / "template_fields.json"
OPERATORS_FILE = BASE_DIR / "data" / "wq_template_operators" / "template_operators.csv"


def build_wq_knowledge_prompt():
    """
    è¯»å– YAML æ¨¡æ¿ï¼Œå¹¶æ ¹æ® config ä¸­å¯ç”¨çš„æ•°æ®é›†æ„å»ºå­—æ®µã€å­—æ®µç±»å‹ã€æ“ä½œç¬¦ä¿¡æ¯ï¼Œ
    æ¸²æŸ“ inject_wq_knowledge promptã€‚
    """

    # è¯»å–æ¨¡æ¿
    with open(PROMPT_FILE, "r", encoding="utf-8") as f:
        prompt_yaml = yaml.safe_load(f)
    template_str = prompt_yaml.get("inject_wq_knowledge", "")
    if not template_str:
        raise ValueError("inject_wq_knowledge not found in template_generating.yaml")

    # è¯»å–é…ç½®ï¼šå¯ç”¨çš„æ•°æ®é›†
    enabled_datasets = ConfigLoader.get("enabled_field_datasets", [])
    print(f"ğŸ”§ Enabled datasets from config: {enabled_datasets}")

    # =========================================================
    # åŠ è½½å­—æ®µæ–‡ä»¶ï¼ˆä»…é™é…ç½®ä¸­å¯ç”¨çš„ï¼‰
    # =========================================================
    field_dfs = []
    for file in FIELDS_DIR.glob("*.csv"):
        dataset_name = file.stem
        if enabled_datasets and dataset_name not in enabled_datasets:
            continue
        if file.stat().st_size == 0:
            print(f"âš ï¸ Skipping empty file: {file.name}")
            continue

        try:
            df = pd.read_csv(file, dtype=str, keep_default_na=False)
            df["__dataset__"] = dataset_name
            field_dfs.append(df)
        except Exception as e:
            print(f"âŒ Failed to load {file.name}: {e}")

    if not field_dfs:
        raise ValueError("âŒ No valid field CSVs loaded. Check config.enabled_field_datasets.")

    fields_df = pd.concat(field_dfs, ignore_index=True)

    # æ„å»ºå­—æ®µå®šä¹‰ä¿¡æ¯
    fields_info = []
    for _, row in fields_df.iterrows():
        desc = row.get("description", "")
        dtype = row.get("type", "")
        dataset = row.get("__dataset__", "")
        field_str = f"- **{row['id']}** ({dtype}, {dataset}): {desc}"
        fields_info.append(field_str)

    fields_and_definitions = "\n".join(fields_info)

    # =========================================================
    # åŠ è½½å­—æ®µç±»å‹æ˜ å°„ï¼ˆæ¥è‡ª template_fields.jsonï¼‰
    # =========================================================
    if not TEMPLATE_FIELDS_FILE.exists():
        raise FileNotFoundError(f"âŒ template_fields.json not found at {TEMPLATE_FIELDS_FILE}")

    with open(TEMPLATE_FIELDS_FILE, "r", encoding="utf-8") as f:
        template_field_data = json.load(f)

    # template_fields.json æ ¼å¼: { "field_type_name": [list of field ids], ... }
    # ä»…ä¿ç•™å±äºå¯ç”¨æ•°æ®é›†çš„å­—æ®µ
    filtered_field_types = {}

    for ftype_full, ids in template_field_data.items():
        # æå– dataset_nameï¼Œä¾‹å¦‚ä» "</momentum:type:pv1/>" å¾—åˆ° "pv1"
        match = re.search(r":([\w\-]+)\/>$", ftype_full)
        if not match:
            continue
        dataset_name = match.group(1)
        # è‹¥ dataset_name åœ¨å¯ç”¨åˆ—è¡¨ä¸­ï¼Œåˆ™ä¿ç•™
        if enabled_datasets and dataset_name not in enabled_datasets:
            continue
        filtered_field_types[ftype_full] = ids

    # æ¸²æŸ“ field types
    field_types_str = []
    for ftype, fields in filtered_field_types.items():
        field_types_str.append(f"- **{ftype}**: {', '.join(fields)}")
    field_types = "\n".join(field_types_str)

    # =========================================================
    # åŠ è½½æ“ä½œç¬¦æ–‡ä»¶
    # =========================================================
    ops_df = pd.read_csv(OPERATORS_FILE)
    ops_info = []
    op_types_map = {}

    for _, row in ops_df.iterrows():
        op_str = f"- **{row['name']}**: {row['definition']} â€” {row['description']}"
        ops_info.append(op_str)
        op_types_map.setdefault(row['type'], []).append(row['name'])

    operators_and_definitions = "\n".join(ops_info)

    op_types_str = []
    for otype, ops in op_types_map.items():
        op_types_str.append(f"- **</{otype}/>**: {', '.join(ops)}")
    operator_types = "\n".join(op_types_str)

    # =========================================================
    # æ¸²æŸ“æ¨¡æ¿
    # =========================================================
    prompt_filled = (
        template_str
        .replace("{{ fields_and_definitions }}", fields_and_definitions)
        .replace("{{ operators_and_definitions }}", operators_and_definitions)
        .replace("{{ field_types }}", field_types)
        .replace("{{ operator_types }}", operator_types)
    )

    print("âœ… WQ knowledge prompt built successfully.")
    return prompt_filled


def build_check_if_blog_helpful(blog_json_path: str):
    """
    ä»yamlè¯»å–check_if_blog_helpfulæ¨¡æ¿å¹¶ç”¨blog_jsonæ¸²æŸ“
    """
    # 1. è¯»å–yaml
    with open(PROMPT_FILE, "r", encoding="utf-8") as f:
        prompt_yaml = yaml.safe_load(f)

    template_str = prompt_yaml.get("check_if_blog_helpful", "")
    if not template_str:
        raise ValueError("check_if_blog_helpful not found in template_generating.yaml")

    # 2. è¯»å–json
    with open(blog_json_path, "r", encoding="utf-8") as f:
        blog_data = json.load(f)

    # 3. æ‹¼æ¥blog_postæ–‡æœ¬
    # å¯ä»¥æŒ‰éœ€æ±‚æ‹¼æ¥ï¼ˆtitle+description+post_body+commentsï¼‰
    post_text = f"Title: {blog_data.get('title','')}\n\nDescription: {blog_data.get('description','')}\n\nPost Body: {blog_data.get('post_body','')}\n\nComments:\n"
    if blog_data.get("post_comments"):
        for i, c in enumerate(blog_data["post_comments"], 1):
            post_text += f"[{i}] {c}\n"

    # 4. æ›¿æ¢æ¨¡æ¿
    prompt_filled = template_str.replace("{{ blog_post }}", truncate_text(post_text))

    return prompt_filled


def build_blog_to_hypothesis(blog_json_path: str):
    """
    ä»yamlè¯»å–blog_to_hypothesisæ¨¡æ¿å¹¶ç”¨blog_jsonæ¸²æŸ“
    """
    # 1. è¯»å–yaml
    with open(PROMPT_FILE, "r", encoding="utf-8") as f:
        prompt_yaml = yaml.safe_load(f)

    template_str = prompt_yaml.get("blog_to_hypothesis", "")
    if not template_str:
        raise ValueError("blog_to_hypothesis not found in template_generating.yaml")

    # 2. è¯»å–json
    with open(blog_json_path, "r", encoding="utf-8") as f:
        blog_data = json.load(f)

    # 3. æ‹¼æ¥blog_postæ–‡æœ¬
    # å¯ä»¥æŒ‰éœ€æ±‚æ‹¼æ¥ï¼ˆtitle+description+post_body+commentsï¼‰
    post_text = f"Title: {blog_data.get('title','')}\n\nDescription: {blog_data.get('description','')}\n\nPost Body: {blog_data.get('post_body','')}\n\nComments:\n"
    if blog_data.get("post_comments"):
        for i, c in enumerate(blog_data["post_comments"], 1):
            post_text += f"[{i}] {c}\n"

    # 4. æ›¿æ¢æ¨¡æ¿
    prompt_filled = template_str.replace("{{ blog_post }}", truncate_text(post_text))

    return prompt_filled


def build_hypothesis_to_template(hypotheses_json_path: str):
    """
    ä»yamlè¯»å–hypothesis_to_templateæ¨¡æ¿å¹¶ç”¨hypotheses_jsonæ¸²æŸ“
    - ä¼˜å…ˆä½¿ç”¨ template_fields.jsonï¼ˆæ˜ å°„ field types -> [field ids]ï¼‰
    - æ ¹æ® config ä¸­çš„ enabled_datasets è¿‡æ»¤ field types ï¼ˆä» </name:type:dataset/> ä¸­è§£æ datasetï¼‰
    - ä¸ºé˜²æ­¢ token çˆ†ç‚¸ï¼Œå±•ç¤ºæ¯ä¸ªç±»å‹çš„å‰ N ä¸ªç¤ºä¾‹å¹¶æ ‡æ³¨æ€»æ•°
    """
    import re
    from utils.config_loader import ConfigLoader

    # 1. è¯»å–yaml
    with open(PROMPT_FILE, "r", encoding="utf-8") as f:
        prompt_yaml = yaml.safe_load(f)

    template_str = prompt_yaml.get("hypothesis_to_template", "")
    if not template_str:
        raise ValueError("hypothesis_to_template not found in template_generating.yaml")

    # 2. è¯»å–hypotheses json
    with open(hypotheses_json_path, "r", encoding="utf-8") as f:
        hypotheses_data = json.load(f)

    hypotheses_str = json.dumps(hypotheses_data, indent=2, ensure_ascii=False)

    # --- è¯»å–å¹¶è¿‡æ»¤ field types (ä¼˜å…ˆ template_fields.json) ---
    field_types_map = {}

    # è·å–ç”¨æˆ·åœ¨ config ä¸­å¯ç”¨çš„æ•°æ®é›†ï¼ˆå¯ä¸ºé€—å·åˆ†éš”å­—ç¬¦ä¸²æˆ– listï¼‰
    enabled = ConfigLoader.get("enabled_field_datasets")
    enabled_datasets = None
    if enabled:
        if isinstance(enabled, str):
            enabled_datasets = [s.strip() for s in enabled.split(",") if s.strip()]
        elif isinstance(enabled, (list, tuple)):
            enabled_datasets = list(enabled)
        else:
            enabled_datasets = None

    if TEMPLATE_FIELDS_FILE.exists():
        with open(TEMPLATE_FIELDS_FILE, "r", encoding="utf-8") as f:
            template_field_data = json.load(f)

        # è§£æ key æ ¼å¼ </name:type:dataset/> æå– dataset å¹¶æŒ‰ enabled_datasets è¿‡æ»¤
        for ftype_full, ids in template_field_data.items():
            # æå–æœ€åä¸€ä¸ªå†’å·ä¹‹åç›´åˆ° '/>' ä¹‹é—´çš„ dataset åç§°
            m = re.search(r":([^/>]+)\/>$", ftype_full)
            dataset_name = m.group(1) if m else None

            # å¦‚æœç”¨æˆ·æŒ‡å®šäº† enabled_datasetsï¼Œåˆ™åªä¿ç•™åŒ¹é…çš„ dataset
            if enabled_datasets and dataset_name and dataset_name not in enabled_datasets:
                continue

            # ä¿æŒåŸå§‹ id åˆ—è¡¨ï¼ˆlistï¼‰ï¼Œåé¢æ¸²æŸ“æ—¶ä¼šæˆªæ–­æ˜¾ç¤º
            field_types_map[ftype_full] = list(ids)
    else:
        raise FileNotFoundError("âŒ template_fields.json not found.")

    # --- è¯»å–æ“ä½œç¬¦ç±»å‹æ˜ å°„ï¼ˆä¿æŒåŸæ ·ï¼Œä½†åšæˆªæ–­æ˜¾ç¤ºï¼‰ ---
    ops_df = pd.read_csv(OPERATORS_FILE, dtype=str, keep_default_na=False)
    op_types_map = {}
    for _, row in ops_df.iterrows():
        typ = row.get("type", "Other")
        name = row.get("name")
        if name:
            op_types_map.setdefault(typ, []).append(name)

    # --- æ„å»ºå¯è¯»å­—ç¬¦ä¸²ï¼ˆä¸º prompt ï¼‰: å¯¹æ¯ä¸ªç±»å‹åªæ˜¾ç¤ºå‰ N ä¸ªç¤ºä¾‹ä»¥èŠ‚çº¦ token ---
    MAX_EXAMPLES_PER_TYPE = 10000  # æ¯ä¸ªç±»å‹åœ¨ prompt ä¸­å±•ç¤ºçš„æœ€å¤§ç¤ºä¾‹æ•°ï¼ˆå­—æ®µæˆ–æ“ä½œç¬¦ï¼‰
    field_types_str_lines = []
    for ftype, ids in field_types_map.items():
        total = len(ids)
        display_ids = ids[:MAX_EXAMPLES_PER_TYPE]
        suffix = "" if total <= MAX_EXAMPLES_PER_TYPE else f", ... (+{total - MAX_EXAMPLES_PER_TYPE} more)"
        field_types_str_lines.append(f"- **{ftype}** ({total} fields): {', '.join(display_ids)}{suffix}")
    field_types = "\n".join(field_types_str_lines)

    op_types_str_lines = []
    for otype, ops in op_types_map.items():
        total = len(ops)
        display_ops = ops[:MAX_EXAMPLES_PER_TYPE]
        suffix = "" if total <= MAX_EXAMPLES_PER_TYPE else f", ... (+{total - MAX_EXAMPLES_PER_TYPE} more)"
        op_types_str_lines.append(f"- **</{otype}/>** ({total} ops): {', '.join(display_ops)}{suffix}")
    operator_types = "\n".join(op_types_str_lines)

    # 4. æ›¿æ¢æ¨¡æ¿
    prompt_filled = (
        template_str
        .replace("{{ hypotheses }}", hypotheses_str)
        .replace("{{ field_types }}", field_types)
        .replace("{{ operator_types }}", operator_types)
    )

    return prompt_filled




if __name__ == "__main__":
    print(build_wq_knowledge_prompt())
